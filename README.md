
# Washing Machine IoT Data ETL Pipeline

This project implements an **ETL (Extract, Transform, Load) pipeline** for handling IoT data generated by smart washing machines. The pipeline processes raw data stored in **AWS S3**, cleans and transforms it using **PySpark**, and makes it queryable via **Amazon Athena**.

---

## ğŸ“Œ Project Overview

The goal of this project is to build a data engineering pipeline that:

1. **Extracts** raw IoT washing machine data from S3.  
2. **Transforms** it into a clean, structured format using PySpark.  
3. **Loads** the cleaned data back into S3 in **Parquet format** for efficient querying.  
4. Uses **AWS Athena** to analyze and visualize the data with SQL.  
5. Provides a reusable and scalable architecture for IoT data processing.

---

## ğŸ“‚ Project Structure

```
washing-machine-etl/
â”‚â”€â”€ etl_job.py              # PySpark ETL script
â”‚â”€â”€ sql_queries/            # Athena SQL queries
â”‚â”€â”€ diagrams/               # ETL diagrams and flowcharts
â”‚â”€â”€ data/                   # Example dataset (if small)
â”‚â”€â”€ README.md               # Project documentation
```

---

## âš™ï¸ Technologies Used

- **AWS S3** â†’ Data lake storage (raw + processed data)
- **AWS Glue / PySpark** â†’ Data cleaning & transformation
- **AWS Athena** â†’ SQL queries on transformed data
- **Parquet Format** â†’ Optimized storage format for analytics
- **GitHub** â†’ Version control and sharing

---

## ğŸ”„ ETL Flow

![ETL Diagram](diagrams/etl_pipeline.png)

1. **Extract**
   - Raw IoT washing machine logs stored in `s3://washing-machine-raw-data/`.

2. **Transform**
   - PySpark script reads raw CSV/JSON files.
   - Handles malformed records (`HIVE_BAD_DATA`).
   - Cleans null values, fixes schema mismatches.
   - Converts cleaned data into Parquet format.

3. **Load**
   - Writes transformed data into `s3://washing-machine-cleaned-data/`.
   - Partitioned by `year`, `month`, `day` for faster queries.

4. **Query (Athena)**
   - Athena tables created on top of the processed data.
   - Sample SQL queries to analyze session counts, usage trends, and error logs.

---

## ğŸš€ How to Run

### 1. Setup AWS Environment
- Create S3 buckets:
  - `washing-machine-raw-data`
  - `washing-machine-cleaned-data`
- Enable **AWS Glue / PySpark** environment.

### 2. Run ETL Job
```bash
spark-submit etl_job.py
```

### 3. Query with Athena
Example query:
```sql
SELECT session_id, COUNT(*) as event_count
FROM washing_machine_cleaned
GROUP BY session_id
ORDER BY event_count DESC;
```

---

## ğŸ›  Improvements & Future Work

- âœ… Automate ETL with **AWS Glue Jobs + Triggers**
- âœ… Add **Airflow orchestration** for scheduling
- âœ… Integrate **QuickSight / Power BI** dashboards
- âœ… Implement **real-time streaming pipeline** with **Kinesis + Spark Structured Streaming**

---
 
![image alt](https://github.com/aHemanth123/AWS_ETL_Pipeline_of-Washing_Machine/blob/63dbaf817b00757f912b2aa0aafbba260dc63da7/Washing_PBI_1.png)
![image alt](https://github.com/aHemanth123/AWS_ETL_Pipeline_of-Washing_Machine/blob/8a3f937e59081943bef609dc465297b09fa9dfb1/Washing_PBI_2.png)
![image alt](https://github.com/aHemanth123/AWS_ETL_Pipeline_of-Washing_Machine/blob/7e5b288881ff6da7781914075cb6154acd2bc731/Washing_PBI_3.png)
